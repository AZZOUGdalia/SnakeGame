# ðŸ Snake Game + SARSA Reinforcement Learning

Ce projet combine un **jeu Snake classique** (jouable avec `turtle`) et un **agent SARSA** qui apprend Ã  jouer **automatiquement** en utilisant les principes fondamentaux du **Reinforcement Learning (RL)**.

---

## Partie 1 : Le jeu Snake
Le fichier [`snake_game.py`](snake_game.py) contient le jeu Snake original, contrÃ´lable au clavier :

- `Z / S / Q / D` â†’ se dÃ©placer (haut / bas / gauche / droite)  
- `Ã‰chap` â†’ quitter le jeu  

Le joueur contrÃ´le un serpent qui grandit en mangeant de la nourriture rouge.  
Chaque collision avec un mur ou son propre corps redÃ©marre la partie.  
Le **score** et le **high score** sont affichÃ©s Ã  lâ€™Ã©cran.

---

## Partie 2 : Lâ€™agent SARSA (On-Policy RL)

Le fichier [`sarsa_agent.py`](sarsa_agent.py) contient un **agent SARSA** qui apprend Ã  jouer sur une grille abstraite (simulation du jeu).

### Principe de lâ€™algorithme SARSA
SARSA (Stateâ€“Actionâ€“Rewardâ€“Stateâ€“Action) est une mÃ©thode **on-policy**, câ€™est-Ã -dire que :
> Lâ€™agent apprend en suivant **la mÃªme politique Îµ-greedy** quâ€™il utilise pour agir.

Formule de mise Ã  jour :
\[
Q(s,a) \leftarrow Q(s,a) + \alpha \big[ r + \gamma Q(s',a') - Q(s,a) \big]
\]

oÃ¹ :
- **Î± (alpha)** â†’ taux dâ€™apprentissage  
- **Î³ (gamma)** â†’ facteur de discount (poids du futur)  
- **Îµ (epsilon)** â†’ taux dâ€™exploration (probabilitÃ© de choisir une action alÃ©atoire)

---

## RÃ©sultats dâ€™apprentissage

### DÃ©croissance de lâ€™exploration
Lâ€™agent commence par **explorer (Îµ=0.1)**, puis devient plus **dÃ©terministe (Îµ=0.02)** :

![epsilon_decay](epsilon_decay.png)

### Performance (Return par Ã©pisode)
On observe une amÃ©lioration progressive du score cumulÃ© :
![return_per_episode](return_per_episode.png)


### Survie (Steps par Ã©pisode)
Le nombre dâ€™Ã©tapes avant la mort du serpent augmente globalement :
![steps_per_episode](steps_per_episode.png)

---

## HyperparamÃ¨tres utilisÃ©s (possibilitÃ© de modifier)

| ParamÃ¨tre | Valeur | RÃ´le |
|------------|---------|------|
| `ALPHA` | 0.2 | Taux dâ€™apprentissage |
| `GAMMA` | 0.95 | PondÃ©ration du futur |
| `EPSILON` | 0.1 | Exploration initiale |
| `EPS_DECAY` | 0.995 | RÃ©duction de lâ€™exploration |
| `EPS_MIN` | 0.02 | Exploration minimale |
| `EPISODES` | 300 | Nombre dâ€™Ã©pisodes dâ€™apprentissage |

---

## ðŸ§¾ Exemple de sortie console

